{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a5aefd-64fd-4a24-b64f-454432646301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from multiprocessing import Process, Manager, Lock\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f899eb-06d6-41c8-af16-bb0cb2e629fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.' # '../data/synthetic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c89e53-6d2d-44ed-a9b3-1251c214439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(53711)\n",
    "np.random.seed(15213)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0ea3b-7207-4c58-b3ba-a6c7188374ff",
   "metadata": {},
   "source": [
    "## Expr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2082c0-ce89-48e3-a497-d64905a4ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'synthetic1'\n",
    "\n",
    "# Parameters\n",
    "dataset_size = 400_000  # Expected size of the dataset\n",
    "means = [(800, 600), (1200, 400) , (100, 1900), (0, 3700)]  # Mean frequency for all datasets\n",
    "std_devs = [100, 200, 300, 400]  # Four different standard deviations\n",
    "query_counts = [random.randint(227, 248) for _ in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6adb08-9b30-4056-a321-507bf5cbf862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_key():\n",
    "    \"\"\"Generate a random query key with length between 3 and 8 inclusively.\"\"\"\n",
    "    key_length = random.randint(3, 8)\n",
    "    return ''.join(random.choices(string.ascii_uppercase, k=key_length))\n",
    "\n",
    "def generate_query():\n",
    "    \"\"\"Generate a query with 3 query keys and 2 regex gap constraints.\"\"\"\n",
    "    # Generate 3 query keys\n",
    "    key1 = generate_query_key()\n",
    "    key2 = generate_query_key()\n",
    "    key3 = generate_query_key()\n",
    "\n",
    "    # Generate 2 random gap constraints\n",
    "    gap1_lower = int(bool(random.getrandbits(1)))\n",
    "    gap1_upper = random.randint(1, 50)\n",
    "    gap2_lower = int(bool(random.getrandbits(1)))\n",
    "    gap2_upper = random.randint(1, 50)\n",
    "\n",
    "    # Format the query with regex gaps\n",
    "    query = f\"{key1}(.{{{gap1_lower},{gap1_upper}}}){key2}(.{{{gap2_lower},{gap2_upper}}}){key3}\"\n",
    "    return query\n",
    "\n",
    "def generate_query_workload(query_count):\n",
    "    \"\"\"Generate a workload of queries.\"\"\"\n",
    "    return [generate_query() for _ in range(query_count)]\n",
    "\n",
    "def generate_trigrams():\n",
    "    \"\"\"Generate all unique trigrams from A-Z.\"\"\"\n",
    "    alphabet = string.ascii_uppercase  # A-Z\n",
    "    trigrams = [f\"{a}{b}{c}\" for a in alphabet for b in alphabet for c in alphabet]\n",
    "    return trigrams  # Use the unique trigrams\n",
    "\n",
    "def generate_dataset(trigrams, trigram_frequencies, dataset_size, lock, shared_dataset, index):\n",
    "    \"\"\"Generate a dataset dynamically based on trigram frequencies.\"\"\"\n",
    "    dataset = []\n",
    "    trigram_counter = Counter()  # Track the appearance of each trigram\n",
    "\n",
    "    # Generate strings until the dataset reaches the desired size\n",
    "    while len(dataset) < dataset_size:\n",
    "        current_string = []\n",
    "\n",
    "        # Randomly select trigrams until the dataset is complete\n",
    "        while True:\n",
    "            # Select available trigrams based on remaining frequency\n",
    "            available_trigrams = [\n",
    "                t for t in trigrams if trigram_counter[t] < trigram_frequencies[t]\n",
    "            ]\n",
    "            if not available_trigrams:\n",
    "                break  # No more trigrams can be selected\n",
    "\n",
    "            # Randomly select a trigram to add to the current string\n",
    "            next_trigram = random.choice(available_trigrams)\n",
    "            current_string.append(next_trigram)\n",
    "            trigram_counter[next_trigram] += 1\n",
    "\n",
    "            # End string generation with some probability to avoid infinite strings\n",
    "            if random.random() < 0.3:\n",
    "                break\n",
    "\n",
    "        # Ensure the string contains at least one trigram\n",
    "        if current_string:\n",
    "            dataset.append(''.join(current_string))\n",
    "\n",
    "    # Save dataset to the shared manager list\n",
    "    with lock:\n",
    "        shared_dataset[index] = (dataset, trigram_counter)\n",
    "\n",
    "def generate_frequencies(trigrams, mean1, mean2, std_dev):\n",
    "    \"\"\"Generate target frequencies for the trigrams using a Normal distribution.\"\"\"\n",
    "    size1 = int(len(trigrams)*random.uniform(0.9, 0.6)) # larger\n",
    "    size2 = len(trigrams) - size1 # smaller\n",
    "    frequencies1 = np.random.normal(loc=mean1, scale=std_dev, size=size1)\n",
    "    frequencies2 = np.random.normal(loc=mean2, scale=std_dev, size=size2)\n",
    "    \n",
    "    # Ensure positive integer frequencies and avoid frequencies of 0\n",
    "    frequencies1 = np.clip(np.abs(frequencies1).astype(int), 1, None)\n",
    "    frequencies2 = np.clip(np.abs(frequencies2).astype(int), 1, None)\n",
    "        \n",
    "    frequencies = np.append(frequencies1, frequencies2)\n",
    "    np.random.shuffle(frequencies)\n",
    "    return dict(zip(trigrams, frequencies))\n",
    "\n",
    "def generate_expr1():\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    # Generate all trigrams\n",
    "    trigrams = generate_trigrams()\n",
    "\n",
    "    # Shared manager object to store datasets\n",
    "    manager = Manager()\n",
    "    shared_dataset = manager.list([None] * 4)  # Store 4 datasets\n",
    "    lock = Lock()\n",
    "\n",
    "    # Create and start processes to generate multiple datasets in parallel\n",
    "    processes = []\n",
    "    for i in range(4):\n",
    "        frequencies = generate_frequencies(trigrams, means[i][0], means[i][1], std_devs[i])\n",
    "        p = Process(target=generate_dataset, args=(\n",
    "            trigrams, frequencies, dataset_size, lock, shared_dataset, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    shared_queries = []\n",
    "    # Generate query workloads\n",
    "    for query_count in query_counts:\n",
    "        queries = generate_query_workload(query_count)\n",
    "        shared_queries.append(queries)\n",
    "        print(len(shared_queries[-1]))\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Print summary of datasets and query workloads\n",
    "    for i, (dataset, trigram_counter) in enumerate(shared_dataset):\n",
    "        print(f\"Dataset {i + 1} with std_dev {std_devs[i]}: {len(dataset)} strings\")\n",
    "        print(f\"Top 10 Trigrams (by frequency): {trigram_counter.most_common(10)}\")\n",
    "        print(f\"Bottom 10 Trigrams (by frequency): {trigram_counter.most_common()[:-10-1:-1]}\")\n",
    "        print(f\"Query Workload {i + 1}: {len(shared_queries[i])} queries\")\n",
    "        print(f\"Sample Query: {shared_queries[i][0]}\")\n",
    "        with open(os.path.join(directory_path, f'data_{i}_std{std_devs[i]}.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        with open(os.path.join(directory_path, f'query_{i}.pkl'), 'wb') as f:\n",
    "            pickle.dump(shared_queries[i], f)\n",
    "            \n",
    "    datasets = [ dataset for dataset, trigram_counter in shared_dataset ]\n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_DIR, 'expr1'), exist_ok=True)\n",
    "    for i in range(4):\n",
    "        query_fn = os.path.join(DATA_DIR, 'expr1', f'query_{i}.txt')\n",
    "        if not os.path.exists(query_fn):\n",
    "            with open(query_fn, 'w') as f:\n",
    "                for line in shared_queries[i]:\n",
    "                    f.write(line + '\\n')\n",
    "        data_fn = os.path.join(DATA_DIR, 'expr1', f'data_{i}_std{std_devs[i]}.txt')\n",
    "        if not os.path.exists(data_fn):\n",
    "            with open(data_fn, 'w') as f:\n",
    "                for line in datasets[i]:\n",
    "                    f.write(line + '\\n')\n",
    "    return datasets, shared_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd02d6f-7c67-45ba-a7d9-e2096ff04619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset, trigrams):\n",
    "    \"\"\"\n",
    "    Analyze the dataset to count how many strings each trigram appears in.\n",
    "    \"\"\"\n",
    "    trigram_in_line = Counter()  # Track which trigrams appear in each string\n",
    "\n",
    "    # Iterate over each string in the dataset and count trigram occurrences per string\n",
    "    for line in dataset:\n",
    "        unique_trigrams = set(\n",
    "            line[i:i + 3] for i in range(len(line) - 2) if line[i:i + 3] in trigrams\n",
    "        )\n",
    "        for trigram in unique_trigrams:\n",
    "            trigram_in_line[trigram] += 1\n",
    "\n",
    "    return trigram_in_line\n",
    "\n",
    "def plot_histogram(trigram_counter, dataset_index, std_dev_val, output_dir):\n",
    "    \"\"\"\n",
    "    Plot a histogram for the distribution of trigrams across strings.\n",
    "    \"\"\"\n",
    "    # Extract the frequency of each trigram (i.e., how many strings contain it)\n",
    "    frequencies = list(trigram_counter.values())\n",
    "\n",
    "    # Define bins for the histogram\n",
    "    bins = np.arange(0, max(frequencies) + 5, 5)  # Binning every 5 occurrences\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(frequencies, bins=bins, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Number of Lines Appears')\n",
    "    plt.ylabel('Number of Trigrams')\n",
    "    plt.title(f'Trigram Distribution std_dev={std_dev_val}')\n",
    "\n",
    "    # Save the plot as a PDF\n",
    "    output_file = os.path.join(output_dir, f'dataset_{dataset_index + 1}_histogram.pdf')\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def analysis_expr1(datasets, trigrams):\n",
    "    \"\"\"\n",
    "    Analyze all datasets and generate histogram plots.\n",
    "    \"\"\"\n",
    "    output_dir = 'histogram_plots'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "    # Analyze each dataset and generate corresponding histograms\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        trigram_counter = analyze_dataset(dataset, trigrams)\n",
    "        plot_histogram(trigram_counter, i,std_devs[i], output_dir)\n",
    "\n",
    "    print(f'All histograms saved to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a1953-c98c-4bc3-977b-251f7927d693",
   "metadata": {},
   "source": [
    "#### datasets, shared_queries = generate_expr1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e8e260-3e7b-4747-8f55-b7144d8b673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.isdir(directory_path):\n",
    "#     datasets, shared_queries = generate_expr1()\n",
    "# else:\n",
    "#     datasets = []\n",
    "#     shared_queries = []\n",
    "#     for i in range(4):\n",
    "#         with open(os.path.join(directory_path, f'data_{i}_std{std_devs[i]}.pkl'), 'rb') as f:\n",
    "#             datasets.append(pickle.load(f))\n",
    "#         with open(os.path.join(directory_path, f'query_{i}.pkl'), 'rb') as f:\n",
    "#             shared_queries.append(pickle.load(f))\n",
    "# analysis_expr1(datasets, trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0928b7-978d-4dd1-8dd0-f22d14e9c20b",
   "metadata": {},
   "source": [
    "## Expr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae6f61d7-a8c5-45e6-820c-c3b13c2f5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_expr2(num_strings, string_length=450):\n",
    "    \"\"\"Generate a dataset with specified number of strings, each of fixed length.\"\"\"\n",
    "    alphabet = string.ascii_uppercase  # English uppercase alphabet\n",
    "    dataset = [\n",
    "        ''.join(random.choices(alphabet, k=string_length))\n",
    "        for _ in range(num_strings)\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "def sample_dataset(dataset, sample_percentage):\n",
    "    \"\"\"Take a sample of the dataset with the specified percentage.\"\"\"\n",
    "    sample_size = int(len(dataset) * sample_percentage)\n",
    "    return random.sample(dataset, sample_size)\n",
    "\n",
    "def generate_query_workload(sample, query_count, lower_char_count, upper_char_count):\n",
    "    \"\"\"Generate query workload from the given sample.\"\"\"\n",
    "    queries = []\n",
    "\n",
    "    for _ in range(query_count):\n",
    "        # Select a random string from the sample\n",
    "        random_string = random.choice(sample)\n",
    "\n",
    "        # Ensure the string has at least lower characters for meaningful slicing\n",
    "        while len(random_string) < lower_char_count+1:\n",
    "            random_string = random.choice(sample)\n",
    "\n",
    "        # Choose a random slice of characters from the string\n",
    "        slice1_start = random.randint(0, len(random_string) - lower_char_count)\n",
    "        slice1_length = random.randint(lower_char_count, upper_char_count)\n",
    "        slice1 = random_string[slice1_start:slice1_start + min(slice1_length, len(random_string)-1)]\n",
    "\n",
    "        # Decide on a random gap size\n",
    "        gap_size = random.randint(1, max(1, min(50, len(random_string) - len(slice1) - 1)))\n",
    "        \n",
    "        # Choose another slice of 3-8 characters after the gap\n",
    "        slice2_start = slice1_start + slice1_length + gap_size\n",
    "        if slice2_start + upper_char_count < len(random_string):  # Ensure valid slice range\n",
    "            slice2_length = random.randint(lower_char_count, upper_char_count)\n",
    "            slice2 = random_string[slice2_start:slice2_start + slice2_length]\n",
    "        else:\n",
    "            slice2 = random_string[slice2_start:]  # Handle edge case where slice2 is out of range\n",
    "        \n",
    "        # Create the query string with a regex-style gap\n",
    "        query = f\"{slice1}(.{{0,{gap_size}}}){slice2}\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "def save_dataset(dataset, filename):\n",
    "    \"\"\"Save the dataset to a file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in dataset:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "def save_queries(queries, filename):\n",
    "    \"\"\"Save the query workload to a file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for query in queries:\n",
    "            f.write(query + '\\n')\n",
    "\n",
    "def generate_and_save_datasets(data_dir):\n",
    "    \"\"\"Generate datasets and save them to files.\"\"\"\n",
    "    dataset_sizes = [20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "\n",
    "    for size in dataset_sizes:\n",
    "        dataset = generate_dataset_expr2(size)\n",
    "        save_dataset(dataset, os.path.join(data_dir, f'dataset_{size}.txt'))\n",
    "\n",
    "def generate_and_save_query_workloads(query_dir, data_dir):\n",
    "    \"\"\"Generate query workloads from datasets and save them.\"\"\"\n",
    "    query_sizes = [100, 500, 2_000, 2_500, 5_000]\n",
    "\n",
    "    # Load the 20K dataset to generate workloads\n",
    "    with open(os.path.join(data_dir, 'dataset_20000.txt'), 'r') as f:\n",
    "        dataset_20k = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for query_size in query_sizes:\n",
    "        sample = sample_dataset(dataset_20k, 0.1)  # Take 10% sample\n",
    "        queries = generate_query_workload(sample, query_size, 3, 8)\n",
    "        save_queries(queries, os.path.join(query_dir, f'query_workload_{query_size}.txt'))\n",
    "\n",
    "def generate_and_save_fixed_workload(query_dir, data_dir):\n",
    "    \"\"\"Generate a fixed 1000-query workload for all datasets.\"\"\"\n",
    "    # Generate workloads for the original datasets\n",
    "    dataset_sizes = [20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "\n",
    "    for size in dataset_sizes:\n",
    "        with open(os.path.join(data_dir, f'dataset_{size}.txt'), 'r') as f:\n",
    "            dataset = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        sample = sample_dataset(dataset, 0.1)  # Take 10% sample\n",
    "        queries = generate_query_workload(sample, 1000, 3, 8)  # Fixed 1000 queries\n",
    "        save_queries(queries, os.path.join(query_dir, f'query_workload_1000_for_{size}.txt'))\n",
    "\n",
    "def generate_expr2():\n",
    "    query_dir = os.path.join(DATA_DIR, 'expr2', 'queries')\n",
    "    os.makedirs(query_dir, exist_ok=True)\n",
    "    data_dir = os.path.join(DATA_DIR, 'expr2', 'datasets')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    # Generate datasets\n",
    "    print(\"Generating datasets...\")\n",
    "    generate_and_save_datasets(data_dir)\n",
    "\n",
    "    # Generate query workloads with fixed 20K dataset\n",
    "    print(\"Generating query workloads with fixed 20K dataset...\")\n",
    "    generate_and_save_query_workloads(query_dir, data_dir)\n",
    "\n",
    "    # Generate fixed 1000-query workloads for all datasets\n",
    "    print(\"Generating fixed 1000-query workloads...\")\n",
    "    generate_and_save_fixed_workload(query_dir, data_dir)\n",
    "\n",
    "    print(\"All datasets and query workloads generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7450d97e-e07e-4f93-b5d9-215bfd4c6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_expr2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306b7e-de36-4077-b316-0e1c1b1f653a",
   "metadata": {},
   "source": [
    "## Expr 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd04350e-626b-48bb-afa0-1a5a305bd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_expr4(alphabet, num_records=5000):\n",
    "    \"\"\"Generate a dataset of strings using the given alphabet.\"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        current_string = []\n",
    "        s = len(alphabet)  # Alphabet size\n",
    "\n",
    "        # Generate the string character-by-character with 1/(2*s) chance of stopping\n",
    "        while True:\n",
    "            char = random.choice(alphabet)\n",
    "            current_string.append(char)\n",
    "\n",
    "            # End generation with a 1/(2*s) probability after adding a character\n",
    "            if random.random() < 1 / (10 * s):\n",
    "                break\n",
    "\n",
    "        dataset.append(''.join(current_string))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_expr4():\n",
    "    # Define alphabets for Rob datasets\n",
    "    rob_alphabets = {\n",
    "        'Rob01': string.ascii_uppercase[:4],  # A-D\n",
    "        'Rob02': string.ascii_uppercase[:8],  # A-H\n",
    "        'Rob03': string.ascii_uppercase[:12],  # A-L\n",
    "        'Rob04': string.ascii_uppercase[:16],  # A-P\n",
    "    }\n",
    "\n",
    "    data_dir = os.path.join('expr4', 'datasets')\n",
    "    query_dir = os.path.join('expr4', 'queries')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "    # Generate datasets for Rob01 to Rob04\n",
    "    for rob_name, alphabet in rob_alphabets.items():\n",
    "        dataset = generate_dataset_expr4(alphabet)\n",
    "        save_dataset(dataset, os.path.join(data_dir, f'{rob_name}.txt'))\n",
    "\n",
    "        # Generate 10%, 30%, and 50% samples and their query workloads\n",
    "        for sample_pct in [0.1, 0.3, 0.5]:\n",
    "            sample = sample_dataset(dataset, sample_pct)\n",
    "            queries = generate_query_workload(sample, int(len(dataset) * sample_pct), 3, 8)\n",
    "            save_queries(queries, os.path.join(query_dir, f'{rob_name}_queries_{int(sample_pct*100)}pct.txt'))\n",
    "        # Generate 2% test query set\n",
    "        test_sample = sample_dataset(dataset, 0.02)\n",
    "        test_queries = generate_query_workload(sample, int(len(dataset) * 0.02), 3, 8)\n",
    "        save_queries(test_queries, os.path.join(query_dir, f'{rob_name}_test_queries_2pct.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d03ecb8-d8d9-4a1d-a3af-175bda6d0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_expr4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc07f1-9f91-4776-88db-45f8adcd5700",
   "metadata": {},
   "source": [
    "# Expr 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf3679-e6e4-4494-9d7b-ef9292e8b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_expr5(alphabet, num_records=5000):\n",
    "    \"\"\"Generate a dataset of strings using the given alphabet.\"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        current_string = []\n",
    "        s = len(alphabet)  # Alphabet size\n",
    "\n",
    "        # Generate the string character-by-character with 1/(2*s) chance of stopping\n",
    "        while True:\n",
    "            char = random.choice(alphabet)\n",
    "            current_string.append(char)\n",
    "\n",
    "            if random.random() < 1 / (10 * s):\n",
    "                break\n",
    "\n",
    "        dataset.append(''.join(current_string))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_query_workload5(sample, query_count, lower_char_count, upper_char_count):\n",
    "    \"\"\"Generate query workload from the given sample.\"\"\"\n",
    "    queries = []\n",
    "\n",
    "    for _ in range(query_count):\n",
    "        # Select a random string from the sample\n",
    "        random_string = random.choice(sample)\n",
    "\n",
    "        # Ensure the string has at least lower characters for meaningful slicing\n",
    "        while len(random_string) < 3*lower_char_count+1:\n",
    "            random_string = random.choice(sample)\n",
    "\n",
    "        # Choose a random slice of characters from the string\n",
    "        slice1_start = random.randint(0, len(random_string) - 3*lower_char_count-1)\n",
    "        slice1_length = random.randint(lower_char_count, upper_char_count)\n",
    "        slice1 = random_string[slice1_start:slice1_start + min(slice1_length, len(random_string)-1)]\n",
    "\n",
    "        # Decide on a random gap size\n",
    "        gap_size1 = random.randint(1, max(1, min(50, len(random_string) - len(slice1) - 1)))\n",
    "        \n",
    "        # Choose another slice of 3-8 characters after the gap\n",
    "        slice2_start = slice1_start + slice1_length + gap_size1\n",
    "        slice2_length = 0\n",
    "        if slice2_start + upper_char_count < len(random_string):  # Ensure valid slice range\n",
    "            slice2_length = random.randint(lower_char_count, upper_char_count)\n",
    "            slice2 = random_string[slice2_start:slice2_start + slice2_length]\n",
    "        else:\n",
    "            slice2 = random_string[slice2_start:]  # Handle edge case where slice2 is out of range\n",
    "            # Create the query string with a regex-style gap\n",
    "            query = f\"{slice1}(.{{0,{gap_size1}}}){slice2}\"\n",
    "            queries.append(query)\n",
    "            continue\n",
    "            \n",
    "        # Decide on a random gap size\n",
    "        gap_size2 = random.randint(1, max(1, min(50, len(random_string) - len(slice2) - gap_size1 - len(slice1) - 1)))\n",
    "        \n",
    "        # Choose another slice of 3-8 characters after the gap\n",
    "        slice3_start = slice2_start + slice2_length + gap_size2\n",
    "        if slice3_start + upper_char_count < len(random_string):  # Ensure valid slice range\n",
    "            slice3_length = random.randint(lower_char_count, upper_char_count)\n",
    "            slice3 = random_string[slice3_start:slice3_start + slice3_length]\n",
    "        else:\n",
    "            slice3 = random_string[slice3_start:]  # Handle edge case where slice3 is out of range\n",
    "        \n",
    "        # Create the query string with a regex-style gap\n",
    "        query = f\"{slice1}(.{{0,{gap_size1}}}){slice2}(.{{0,{gap_size2}}}){slice3}\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def generate_expr5():\n",
    "    # Define alphabets for Rob datasets\n",
    "    rob_alphabets = {\n",
    "        'Rob01': string.ascii_uppercase[:4],  # A-D\n",
    "        'Rob02': string.ascii_uppercase[:8],  # A-H\n",
    "        'Rob03': string.ascii_uppercase[:12],  # A-L\n",
    "        'Rob04': string.ascii_uppercase[:16],  # A-P\n",
    "    }\n",
    "\n",
    "    data_dir = os.path.join('expr5', 'datasets')\n",
    "    query_dir = os.path.join('expr5', 'queries')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "    # Generate datasets for Rob01 to Rob04\n",
    "    for rob_name, alphabet in rob_alphabets.items():\n",
    "        dataset = generate_dataset_expr5(alphabet)\n",
    "        save_dataset(dataset, os.path.join(data_dir, f'{rob_name}.txt'))\n",
    "\n",
    "        # Generate 10%, 30%, and 50% samples and their query workloads\n",
    "        for sample_pct in [0.1, 0.3, 0.5]:\n",
    "            sample = sample_dataset(dataset, sample_pct)\n",
    "            queries = generate_query_workload5(sample, int(len(dataset) * sample_pct), 3, 8)\n",
    "            save_queries(queries, os.path.join(query_dir, f'{rob_name}_queries_{int(sample_pct*100)}pct.txt'))\n",
    "        # Generate 2% test query set\n",
    "        test_sample = sample_dataset(dataset, 0.02)\n",
    "        test_queries = generate_query_workload(sample, int(len(dataset) * 0.02), 3, 8)\n",
    "        save_queries(test_queries, os.path.join(query_dir, f'{rob_name}_test_queries_2pct.txt'))\n",
    "\n",
    "# generate_expr5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07cbd2",
   "metadata": {},
   "source": [
    "# Comprehensive Configurable Synthetic Dataset Generator\n",
    "\n",
    "This section provides a unified, configurable approach to generate synthetic datasets with varying:\n",
    "- Dataset sizes\n",
    "- Query set sizes  \n",
    "- Average selectivities\n",
    "- Alphabet sizes\n",
    "- String length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31896b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableSyntheticGenerator:\n",
    "    \"\"\"\n",
    "    A comprehensive synthetic dataset generator with configurable parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - alphabet_sizes: List of alphabet sizes (e.g., [4, 8, 12, 16])\n",
    "    - dataset_sizes: List of dataset sizes (e.g., [10000, 50000, 100000])\n",
    "    - query_set_sizes: List of query set sizes (e.g., [100, 500, 1000])\n",
    "    - selectivity_targets: List of target selectivities (e.g., [0.01, 0.05, 0.1])\n",
    "    - string_length_params: Dict with 'min', 'max', 'mean' for string lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.alphabets = {}\n",
    "        self._initialize_alphabets()\n",
    "    \n",
    "    def _initialize_alphabets(self):\n",
    "        \"\"\"Initialize alphabets of different sizes.\"\"\"\n",
    "        full_alphabet = string.ascii_uppercase + string.digits + string.punctuation\n",
    "        for size in self.config['alphabet_sizes']:\n",
    "            self.alphabets[size] = full_alphabet[:size]\n",
    "    \n",
    "    def generate_string(self, alphabet, length_params):\n",
    "        \"\"\"Generate a single string with given alphabet and length parameters.\"\"\"\n",
    "        if 'fixed' in length_params:\n",
    "            length = length_params['fixed']\n",
    "        else:\n",
    "            # Use normal distribution for length\n",
    "            length = max(1, int(np.random.normal(\n",
    "                length_params.get('mean', 100),\n",
    "                length_params.get('std', 20)\n",
    "            )))\n",
    "            length = min(max(length, length_params.get('min', 10)), \n",
    "                        length_params.get('max', 500))\n",
    "        \n",
    "        return ''.join(random.choices(alphabet, k=length))\n",
    "    \n",
    "    def generate_dataset(self, alphabet_size, dataset_size, string_length_params):\n",
    "        \"\"\"Generate a dataset with specified parameters.\"\"\"\n",
    "        alphabet = self.alphabets[alphabet_size]\n",
    "        dataset = []\n",
    "        \n",
    "        for _ in range(dataset_size):\n",
    "            string_item = self.generate_string(alphabet, string_length_params)\n",
    "            dataset.append(string_item)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def calculate_selectivity(self, query_pattern, dataset):\n",
    "        \"\"\"Calculate the actual selectivity of a query pattern in the dataset.\"\"\"\n",
    "        import re\n",
    "        try:\n",
    "            compiled_pattern = re.compile(query_pattern)\n",
    "            matches = sum(1 for string in dataset if compiled_pattern.search(string))\n",
    "            return matches / len(dataset)\n",
    "        except re.error:\n",
    "            return 0.0\n",
    "    \n",
    "    def generate_query_with_target_selectivity(self, dataset, target_selectivity, \n",
    "                                             max_attempts=100):\n",
    "        \"\"\"Generate a query that approximately matches the target selectivity.\"\"\"\n",
    "        alphabet = set(''.join(dataset))\n",
    "        query = \"\"\n",
    "        actual_selectivity = 0.0\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            # Generate query components with varying complexity based on target selectivity\n",
    "            if target_selectivity > 0.1:  # High selectivity - simple patterns\n",
    "                query_length = random.randint(2, 4)\n",
    "                chars = random.choices(list(alphabet), k=query_length)\n",
    "                query = ''.join(chars)\n",
    "            elif target_selectivity > 0.05:  # Medium selectivity - with gaps\n",
    "                chars1 = ''.join(random.choices(list(alphabet), k=random.randint(2, 3)))\n",
    "                chars2 = ''.join(random.choices(list(alphabet), k=random.randint(2, 3)))\n",
    "                gap = random.randint(1, 10)\n",
    "                query = f\"{chars1}(.{{{0},{gap}}}){chars2}\"\n",
    "            else:  # Low selectivity - complex patterns\n",
    "                chars1 = ''.join(random.choices(list(alphabet), k=random.randint(3, 5)))\n",
    "                chars2 = ''.join(random.choices(list(alphabet), k=random.randint(3, 5)))\n",
    "                chars3 = ''.join(random.choices(list(alphabet), k=random.randint(2, 4)))\n",
    "                gap1 = random.randint(1, 20)\n",
    "                gap2 = random.randint(1, 20)\n",
    "                query = f\"{chars1}(.{{{0},{gap1}}}){chars2}(.{{{0},{gap2}}}){chars3}\"\n",
    "            \n",
    "            actual_selectivity = self.calculate_selectivity(query, dataset)\n",
    "            \n",
    "            # Accept if within reasonable range of target\n",
    "            tolerance = target_selectivity * 0.5  # 50% tolerance\n",
    "            if abs(actual_selectivity - target_selectivity) <= tolerance:\n",
    "                return query, actual_selectivity\n",
    "        \n",
    "        # If no good match found, return best attempt\n",
    "        return query, actual_selectivity\n",
    "    \n",
    "    def generate_query_set(self, dataset, query_set_size, target_selectivity):\n",
    "        \"\"\"Generate a set of queries with approximately the target selectivity.\"\"\"\n",
    "        queries = []\n",
    "        actual_selectivities = []\n",
    "        \n",
    "        for _ in range(query_set_size):\n",
    "            query, actual_sel = self.generate_query_with_target_selectivity(\n",
    "                dataset, target_selectivity\n",
    "            )\n",
    "            queries.append(query)\n",
    "            actual_selectivities.append(actual_sel)\n",
    "        \n",
    "        return queries, actual_selectivities\n",
    "    \n",
    "    def generate_comprehensive_benchmark(self, output_dir):\n",
    "        \"\"\"Generate a comprehensive benchmark suite.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for alphabet_size in self.config['alphabet_sizes']:\n",
    "            for dataset_size in self.config['dataset_sizes']:\n",
    "                for target_selectivity in self.config['selectivity_targets']:\n",
    "                    \n",
    "                    print(f\"Generating: alphabet={alphabet_size}, \"\n",
    "                          f\"dataset_size={dataset_size}, \"\n",
    "                          f\"selectivity={target_selectivity}\")\n",
    "                    \n",
    "                    # Generate dataset\n",
    "                    dataset = self.generate_dataset(\n",
    "                        alphabet_size, \n",
    "                        dataset_size, \n",
    "                        self.config['string_length_params']\n",
    "                    )\n",
    "                    \n",
    "                    # Generate query sets of different sizes\n",
    "                    for query_set_size in self.config['query_set_sizes']:\n",
    "                        queries, actual_sels = self.generate_query_set(\n",
    "                            dataset, query_set_size, target_selectivity\n",
    "                        )\n",
    "                        \n",
    "                        # Create unique identifier\n",
    "                        identifier = f\"alph{alphabet_size}_data{dataset_size}_qs{query_set_size}_sel{target_selectivity:.3f}\"\n",
    "                        \n",
    "                        # Save dataset\n",
    "                        dataset_file = os.path.join(output_dir, f\"dataset_{identifier}.txt\")\n",
    "                        with open(dataset_file, 'w') as f:\n",
    "                            for line in dataset:\n",
    "                                f.write(line + '\\n')\n",
    "                        \n",
    "                        # Save queries\n",
    "                        query_file = os.path.join(output_dir, f\"queries_{identifier}.txt\")\n",
    "                        with open(query_file, 'w') as f:\n",
    "                            for query in queries:\n",
    "                                f.write(query + '\\n')\n",
    "                        \n",
    "                        # Record metadata\n",
    "                        avg_actual_selectivity = np.mean(actual_sels)\n",
    "                        results.append({\n",
    "                            'alphabet_size': alphabet_size,\n",
    "                            'dataset_size': dataset_size,\n",
    "                            'query_set_size': query_set_size,\n",
    "                            'target_selectivity': target_selectivity,\n",
    "                            'actual_avg_selectivity': avg_actual_selectivity,\n",
    "                            'dataset_file': dataset_file,\n",
    "                            'query_file': query_file,\n",
    "                            'identifier': identifier\n",
    "                        })\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = os.path.join(output_dir, 'benchmark_metadata.pkl')\n",
    "        with open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        \n",
    "        # Save human-readable summary\n",
    "        summary_file = os.path.join(output_dir, 'benchmark_summary.txt')\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"Synthetic Benchmark Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            for result in results:\n",
    "                f.write(f\"Configuration: {result['identifier']}\\n\")\n",
    "                f.write(f\"  Alphabet Size: {result['alphabet_size']}\\n\")\n",
    "                f.write(f\"  Dataset Size: {result['dataset_size']}\\n\")\n",
    "                f.write(f\"  Query Set Size: {result['query_set_size']}\\n\")\n",
    "                f.write(f\"  Target Selectivity: {result['target_selectivity']:.3f}\\n\")\n",
    "                f.write(f\"  Actual Avg Selectivity: {result['actual_avg_selectivity']:.3f}\\n\")\n",
    "                f.write(f\"  Dataset File: {result['dataset_file']}\\n\")\n",
    "                f.write(f\"  Query File: {result['query_file']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"Generated {len(results)} benchmark configurations in {output_dir}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations for different experimental scenarios\n",
    "\n",
    "# Configuration 1: Small-scale testing\n",
    "small_scale_config = {\n",
    "    'alphabet_sizes': [4, 8, 16],\n",
    "    'dataset_sizes': [1000, 5000, 10000],\n",
    "    'query_set_sizes': [50, 100, 200],\n",
    "    'selectivity_targets': [0.01, 0.05, 0.1, 0.2],\n",
    "    'string_length_params': {\n",
    "        'mean': 100,\n",
    "        'std': 20,\n",
    "        'min': 20,\n",
    "        'max': 200\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 2: Medium-scale evaluation  \n",
    "medium_scale_config = {\n",
    "    'alphabet_sizes': [8, 16, 26, 32],\n",
    "    'dataset_sizes': [10000, 50000, 100000],\n",
    "    'query_set_sizes': [100, 500, 1000],\n",
    "    'selectivity_targets': [0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "    'string_length_params': {\n",
    "        'mean': 150,\n",
    "        'std': 30,\n",
    "        'min': 50,\n",
    "        'max': 300\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 3: Large-scale benchmarking\n",
    "large_scale_config = {\n",
    "    'alphabet_sizes': [16, 26, 52, 64],  # uppercase, upper+lower, alphanumeric, extended\n",
    "    'dataset_sizes': [50000, 100000, 200000, 500000],\n",
    "    'query_set_sizes': [500, 1000, 2000, 5000],\n",
    "    'selectivity_targets': [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "    'string_length_params': {\n",
    "        'mean': 200,\n",
    "        'std': 50,\n",
    "        'min': 50,\n",
    "        'max': 500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 4: Fixed-length strings (similar to existing methods)\n",
    "fixed_length_config = {\n",
    "    'alphabet_sizes': [4, 8, 12, 16, 26],\n",
    "    'dataset_sizes': [20000, 40000, 60000, 80000, 100000],\n",
    "    'query_set_sizes': [100, 500, 1000, 2000],\n",
    "    'selectivity_targets': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'string_length_params': {\n",
    "        'fixed': 450  # Fixed length like in expr2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_configurable_benchmarks():\n",
    "    \"\"\"Generate benchmarks using the configurable generator.\"\"\"\n",
    "    \n",
    "    # Choose configuration based on desired scale\n",
    "    config = large_scale_config  # Change this to test different scales\n",
    "    \n",
    "    # Create generator\n",
    "    generator = ConfigurableSyntheticGenerator(config)\n",
    "    \n",
    "    # Generate comprehensive benchmark\n",
    "    output_dir = os.path.join(DATA_DIR, 'configurable_synthetic')\n",
    "    results = generator.generate_comprehensive_benchmark(output_dir)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nGenerated {len(results)} benchmark configurations\")\n",
    "    print(f\"Alphabet sizes: {sorted(set(r['alphabet_size'] for r in results))}\")\n",
    "    print(f\"Dataset sizes: {sorted(set(r['dataset_size'] for r in results))}\")\n",
    "    print(f\"Query set sizes: {sorted(set(r['query_set_size'] for r in results))}\")\n",
    "    print(f\"Target selectivities: {sorted(set(r['target_selectivity'] for r in results))}\")\n",
    "    \n",
    "    # Analyze selectivity accuracy\n",
    "    selectivity_errors = [\n",
    "        abs(r['actual_avg_selectivity'] - r['target_selectivity']) \n",
    "        for r in results\n",
    "    ]\n",
    "    print(f\"\\nSelectivity targeting accuracy:\")\n",
    "    print(f\"Mean absolute error: {np.mean(selectivity_errors):.4f}\")\n",
    "    print(f\"Max absolute error: {np.max(selectivity_errors):.4f}\")\n",
    "    print(f\"90th percentile error: {np.percentile(selectivity_errors, 90):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_large_scale_benchmarks():\n",
    "    \"\"\"Generate large-scale benchmarks for comprehensive evaluation.\"\"\"\n",
    "    \n",
    "    # Use the large-scale configuration\n",
    "    config = large_scale_config\n",
    "    \n",
    "    # Create generator\n",
    "    generator = ConfigurableSyntheticGenerator(config)\n",
    "    \n",
    "    # Generate comprehensive benchmark\n",
    "    output_dir = os.path.join(DATA_DIR, 'large_scale_synthetic')\n",
    "    results = generator.generate_comprehensive_benchmark(output_dir)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nGenerated {len(results)} large-scale benchmark configurations\")\n",
    "    print(f\"Total configurations: {len(results)}\")\n",
    "    print(f\"Alphabet sizes: {sorted(set(r['alphabet_size'] for r in results))}\")\n",
    "    print(f\"Dataset sizes: {sorted(set(r['dataset_size'] for r in results))}\")\n",
    "    print(f\"Query set sizes: {sorted(set(r['query_set_size'] for r in results))}\")\n",
    "    print(f\"Target selectivities: {sorted(set(r['target_selectivity'] for r in results))}\")\n",
    "    \n",
    "    # Analyze selectivity accuracy\n",
    "    selectivity_errors = [\n",
    "        abs(r['actual_avg_selectivity'] - r['target_selectivity']) \n",
    "        for r in results\n",
    "    ]\n",
    "    print(f\"\\nSelectivity targeting accuracy:\")\n",
    "    print(f\"Mean absolute error: {np.mean(selectivity_errors):.4f}\")\n",
    "    print(f\"Max absolute error: {np.max(selectivity_errors):.4f}\")\n",
    "    print(f\"90th percentile error: {np.percentile(selectivity_errors, 90):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Generate a small test set\n",
    "def generate_test_benchmark():\n",
    "    \"\"\"Generate a small test benchmark for validation.\"\"\"\n",
    "    test_config = {\n",
    "        'alphabet_sizes': [4, 8],\n",
    "        'dataset_sizes': [1000, 2000],\n",
    "        'query_set_sizes': [50, 100],\n",
    "        'selectivity_targets': [0.05, 0.1],\n",
    "        'string_length_params': {\n",
    "            'mean': 80,\n",
    "            'std': 15,\n",
    "            'min': 30,\n",
    "            'max': 150\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    generator = ConfigurableSyntheticGenerator(test_config)\n",
    "    output_dir = os.path.join(DATA_DIR, 'test_synthetic')\n",
    "    results = generator.generate_comprehensive_benchmark(output_dir)\n",
    "    \n",
    "    print(f\"Generated test benchmark with {len(results)} configurations in {output_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate test benchmark\n",
    "# test_results = generate_test_benchmark()\n",
    "\n",
    "# Uncomment to generate full configurable benchmark (uses large_scale_config now)\n",
    "# results = generate_configurable_benchmarks()\n",
    "\n",
    "# Uncomment to generate large-scale benchmarks specifically\n",
    "# large_results = generate_large_scale_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_from_script(config_name='large_scale_config', output_dir='./configurable_synthetic'):\n",
    "    \"\"\"\n",
    "    Convenient function to generate synthetic data from external scripts.\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of configuration to use ('small_scale_config', 'medium_scale_config', \n",
    "                    'large_scale_config', 'fixed_length_config')\n",
    "        output_dir: Directory to save the generated data\n",
    "    \n",
    "    Returns:\n",
    "        List of generated benchmark results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration mapping\n",
    "    configs = {\n",
    "        'small_scale_config': small_scale_config,\n",
    "        'medium_scale_config': medium_scale_config, \n",
    "        'large_scale_config': large_scale_config,\n",
    "        'fixed_length_config': fixed_length_config\n",
    "    }\n",
    "    \n",
    "    if config_name not in configs:\n",
    "        raise ValueError(f\"Unknown config: {config_name}. Available: {list(configs.keys())}\")\n",
    "    \n",
    "    config = configs[config_name]\n",
    "    \n",
    "    print(f\"Generating synthetic data with {config_name}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Create generator and run\n",
    "    generator = ConfigurableSyntheticGenerator(config)\n",
    "    results = generator.generate_comprehensive_benchmark(output_dir)\n",
    "    \n",
    "    print(f\"Successfully generated {len(results)} benchmark configurations\")\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = generate_synthetic_data_from_script('small_scale_config', './test_synthetic')\n",
    "results = generate_synthetic_data_from_script('large_scale_config', './configurable_synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_synthetic_benchmark(results_file):\n",
    "    \"\"\"Analyze the generated synthetic benchmark.\"\"\"\n",
    "    with open(results_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    \n",
    "    # Group by different parameters\n",
    "    by_alphabet = defaultdict(list)\n",
    "    by_dataset_size = defaultdict(list)\n",
    "    by_query_size = defaultdict(list)\n",
    "    by_selectivity = defaultdict(list)\n",
    "    \n",
    "    for result in results:\n",
    "        by_alphabet[result['alphabet_size']].append(result)\n",
    "        by_dataset_size[result['dataset_size']].append(result)\n",
    "        by_query_size[result['query_set_size']].append(result)\n",
    "        by_selectivity[result['target_selectivity']].append(result)\n",
    "    \n",
    "    print(\"Analysis of Synthetic Benchmark\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nBy Alphabet Size:\")\n",
    "    for alph_size, configs in sorted(by_alphabet.items()):\n",
    "        selectivities = [c['actual_avg_selectivity'] for c in configs]\n",
    "        print(f\"  {alph_size}: {len(configs)} configs, \"\n",
    "              f\"selectivity range [{min(selectivities):.4f}, {max(selectivities):.4f}]\")\n",
    "    \n",
    "    print(f\"\\nBy Dataset Size:\")\n",
    "    for data_size, configs in sorted(by_dataset_size.items()):\n",
    "        selectivities = [c['actual_avg_selectivity'] for c in configs]\n",
    "        print(f\"  {data_size}: {len(configs)} configs, \"\n",
    "              f\"selectivity range [{min(selectivities):.4f}, {max(selectivities):.4f}]\")\n",
    "    \n",
    "    print(f\"\\nBy Query Set Size:\")\n",
    "    for query_size, configs in sorted(by_query_size.items()):\n",
    "        selectivities = [c['actual_avg_selectivity'] for c in configs]\n",
    "        print(f\"  {query_size}: {len(configs)} configs, \"\n",
    "              f\"selectivity range [{min(selectivities):.4f}, {max(selectivities):.4f}]\")\n",
    "    \n",
    "    print(f\"\\nSelectivity Target vs Actual:\")\n",
    "    for target_sel, configs in sorted(by_selectivity.items()):\n",
    "        actual_sels = [c['actual_avg_selectivity'] for c in configs]\n",
    "        mean_actual = np.mean(actual_sels)\n",
    "        std_actual = np.std(actual_sels)\n",
    "        print(f\"  Target {target_sel:.3f}: \"\n",
    "              f\"Actual {mean_actual:.4f} ± {std_actual:.4f} \"\n",
    "              f\"(error: {abs(target_sel - mean_actual):.4f})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
