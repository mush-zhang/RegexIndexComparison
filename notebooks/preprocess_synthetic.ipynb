{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a5aefd-64fd-4a24-b64f-454432646301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from multiprocessing import Process, Manager, Lock\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f899eb-06d6-41c8-af16-bb0cb2e629fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.' # '../data/synthetic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c89e53-6d2d-44ed-a9b3-1251c214439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(53711)\n",
    "np.random.seed(15213)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0ea3b-7207-4c58-b3ba-a6c7188374ff",
   "metadata": {},
   "source": [
    "## Expr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2082c0-ce89-48e3-a497-d64905a4ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'synthetic1'\n",
    "\n",
    "# Parameters\n",
    "dataset_size = 400_000  # Expected size of the dataset\n",
    "means = [(800, 600), (1200, 400) , (100, 1900), (0, 3700)]  # Mean frequency for all datasets\n",
    "std_devs = [100, 200, 300, 400]  # Four different standard deviations\n",
    "query_counts = [random.randint(227, 248) for _ in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6adb08-9b30-4056-a321-507bf5cbf862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_key():\n",
    "    \"\"\"Generate a random query key with length between 3 and 8 inclusively.\"\"\"\n",
    "    key_length = random.randint(3, 8)\n",
    "    return ''.join(random.choices(string.ascii_uppercase, k=key_length))\n",
    "\n",
    "def generate_query():\n",
    "    \"\"\"Generate a query with 3 query keys and 2 regex gap constraints.\"\"\"\n",
    "    # Generate 3 query keys\n",
    "    key1 = generate_query_key()\n",
    "    key2 = generate_query_key()\n",
    "    key3 = generate_query_key()\n",
    "\n",
    "    # Generate 2 random gap constraints\n",
    "    gap1_lower = int(bool(random.getrandbits(1)))\n",
    "    gap1_upper = random.randint(1, 50)\n",
    "    gap2_lower = int(bool(random.getrandbits(1)))\n",
    "    gap2_upper = random.randint(1, 50)\n",
    "\n",
    "    # Format the query with regex gaps\n",
    "    query = f\"{key1}(.{{{gap1_lower},{gap1_upper}}}){key2}(.{{{gap2_lower},{gap2_upper}}}){key3}\"\n",
    "    return query\n",
    "\n",
    "def generate_query_workload(query_count):\n",
    "    \"\"\"Generate a workload of queries.\"\"\"\n",
    "    return [generate_query() for _ in range(query_count)]\n",
    "\n",
    "def generate_trigrams():\n",
    "    \"\"\"Generate all unique trigrams from A-Z.\"\"\"\n",
    "    alphabet = string.ascii_uppercase  # A-Z\n",
    "    trigrams = [f\"{a}{b}{c}\" for a in alphabet for b in alphabet for c in alphabet]\n",
    "    return trigrams  # Use the unique trigrams\n",
    "\n",
    "def generate_dataset(trigrams, trigram_frequencies, dataset_size, lock, shared_dataset, index):\n",
    "    \"\"\"Generate a dataset dynamically based on trigram frequencies.\"\"\"\n",
    "    dataset = []\n",
    "    trigram_counter = Counter()  # Track the appearance of each trigram\n",
    "\n",
    "    # Generate strings until the dataset reaches the desired size\n",
    "    while len(dataset) < dataset_size:\n",
    "        current_string = []\n",
    "\n",
    "        # Randomly select trigrams until the dataset is complete\n",
    "        while True:\n",
    "            # Select available trigrams based on remaining frequency\n",
    "            available_trigrams = [\n",
    "                t for t in trigrams if trigram_counter[t] < trigram_frequencies[t]\n",
    "            ]\n",
    "            if not available_trigrams:\n",
    "                break  # No more trigrams can be selected\n",
    "\n",
    "            # Randomly select a trigram to add to the current string\n",
    "            next_trigram = random.choice(available_trigrams)\n",
    "            current_string.append(next_trigram)\n",
    "            trigram_counter[next_trigram] += 1\n",
    "\n",
    "            # End string generation with some probability to avoid infinite strings\n",
    "            if random.random() < 0.3:\n",
    "                break\n",
    "\n",
    "        # Ensure the string contains at least one trigram\n",
    "        if current_string:\n",
    "            dataset.append(''.join(current_string))\n",
    "\n",
    "    # Save dataset to the shared manager list\n",
    "    with lock:\n",
    "        shared_dataset[index] = (dataset, trigram_counter)\n",
    "\n",
    "def generate_frequencies(trigrams, mean1, mean2, std_dev):\n",
    "    \"\"\"Generate target frequencies for the trigrams using a Normal distribution.\"\"\"\n",
    "    size1 = int(len(trigrams)*random.uniform(0.9, 0.6)) # larger\n",
    "    size2 = len(trigrams) - size1 # smaller\n",
    "    frequencies1 = np.random.normal(loc=mean1, scale=std_dev, size=size1)\n",
    "    frequencies2 = np.random.normal(loc=mean2, scale=std_dev, size=size2)\n",
    "    \n",
    "    # Ensure positive integer frequencies and avoid frequencies of 0\n",
    "    frequencies1 = np.clip(np.abs(frequencies1).astype(int), 1, None)\n",
    "    frequencies2 = np.clip(np.abs(frequencies2).astype(int), 1, None)\n",
    "        \n",
    "    frequencies = np.append(frequencies1, frequencies2)\n",
    "    np.random.shuffle(frequencies)\n",
    "    return dict(zip(trigrams, frequencies))\n",
    "\n",
    "def generate_expr1():\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    # Generate all trigrams\n",
    "    trigrams = generate_trigrams()\n",
    "\n",
    "    # Shared manager object to store datasets\n",
    "    manager = Manager()\n",
    "    shared_dataset = manager.list([None] * 4)  # Store 4 datasets\n",
    "    lock = Lock()\n",
    "\n",
    "    # Create and start processes to generate multiple datasets in parallel\n",
    "    processes = []\n",
    "    for i in range(4):\n",
    "        frequencies = generate_frequencies(trigrams, means[i][0], means[i][1], std_devs[i])\n",
    "        p = Process(target=generate_dataset, args=(\n",
    "            trigrams, frequencies, dataset_size, lock, shared_dataset, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    shared_queries = []\n",
    "    # Generate query workloads\n",
    "    for query_count in query_counts:\n",
    "        queries = generate_query_workload(query_count)\n",
    "        shared_queries.append(queries)\n",
    "        print(len(shared_queries[-1]))\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Print summary of datasets and query workloads\n",
    "    for i, (dataset, trigram_counter) in enumerate(shared_dataset):\n",
    "        print(f\"Dataset {i + 1} with std_dev {std_devs[i]}: {len(dataset)} strings\")\n",
    "        print(f\"Top 10 Trigrams (by frequency): {trigram_counter.most_common(10)}\")\n",
    "        print(f\"Bottom 10 Trigrams (by frequency): {trigram_counter.most_common()[:-10-1:-1]}\")\n",
    "        print(f\"Query Workload {i + 1}: {len(shared_queries[i])} queries\")\n",
    "        print(f\"Sample Query: {shared_queries[i][0]}\")\n",
    "        with open(os.path.join(directory_path, f'data_{i}_std{std_devs[i]}.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        with open(os.path.join(directory_path, f'query_{i}.pkl'), 'wb') as f:\n",
    "            pickle.dump(shared_queries[i], f)\n",
    "            \n",
    "    datasets = [ dataset for dataset, trigram_counter in shared_dataset ]\n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_DIR, 'expr1'), exist_ok=True)\n",
    "    for i in range(4):\n",
    "        query_fn = os.path.join(DATA_DIR, 'expr1', f'query_{i}.txt')\n",
    "        if not os.path.exists(query_fn):\n",
    "            with open(query_fn, 'w') as f:\n",
    "                for line in shared_queries[i]:\n",
    "                    f.write(line + '\\n')\n",
    "        data_fn = os.path.join(DATA_DIR, 'expr1', f'data_{i}_std{std_devs[i]}.txt')\n",
    "        if not os.path.exists(data_fn):\n",
    "            with open(data_fn, 'w') as f:\n",
    "                for line in datasets[i]:\n",
    "                    f.write(line + '\\n')\n",
    "    return datasets, shared_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd02d6f-7c67-45ba-a7d9-e2096ff04619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset, trigrams):\n",
    "    \"\"\"\n",
    "    Analyze the dataset to count how many strings each trigram appears in.\n",
    "    \"\"\"\n",
    "    trigram_in_line = Counter()  # Track which trigrams appear in each string\n",
    "\n",
    "    # Iterate over each string in the dataset and count trigram occurrences per string\n",
    "    for line in dataset:\n",
    "        unique_trigrams = set(\n",
    "            line[i:i + 3] for i in range(len(line) - 2) if line[i:i + 3] in trigrams\n",
    "        )\n",
    "        for trigram in unique_trigrams:\n",
    "            trigram_in_line[trigram] += 1\n",
    "\n",
    "    return trigram_in_line\n",
    "\n",
    "def plot_histogram(trigram_counter, dataset_index, std_dev_val, output_dir):\n",
    "    \"\"\"\n",
    "    Plot a histogram for the distribution of trigrams across strings.\n",
    "    \"\"\"\n",
    "    # Extract the frequency of each trigram (i.e., how many strings contain it)\n",
    "    frequencies = list(trigram_counter.values())\n",
    "\n",
    "    # Define bins for the histogram\n",
    "    bins = np.arange(0, max(frequencies) + 5, 5)  # Binning every 5 occurrences\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(frequencies, bins=bins, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Number of Lines Appears')\n",
    "    plt.ylabel('Number of Trigrams')\n",
    "    plt.title(f'Trigram Distribution std_dev={std_dev_val}')\n",
    "\n",
    "    # Save the plot as a PDF\n",
    "    output_file = os.path.join(output_dir, f'dataset_{dataset_index + 1}_histogram.pdf')\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "def analysis_expr1(datasets, trigrams):\n",
    "    \"\"\"\n",
    "    Analyze all datasets and generate histogram plots.\n",
    "    \"\"\"\n",
    "    output_dir = 'histogram_plots'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "    # Analyze each dataset and generate corresponding histograms\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        trigram_counter = analyze_dataset(dataset, trigrams)\n",
    "        plot_histogram(trigram_counter, i,std_devs[i], output_dir)\n",
    "\n",
    "    print(f'All histograms saved to: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a1953-c98c-4bc3-977b-251f7927d693",
   "metadata": {},
   "source": [
    "#### datasets, shared_queries = generate_expr1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8e260-3e7b-4747-8f55-b7144d8b673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.isdir(directory_path):\n",
    "#     datasets, shared_queries = generate_expr1()\n",
    "# else:\n",
    "#     datasets = []\n",
    "#     shared_queries = []\n",
    "#     for i in range(4):\n",
    "#         with open(os.path.join(directory_path, f'data_{i}_std{std_devs[i]}.pkl'), 'rb') as f:\n",
    "#             datasets.append(pickle.load(f))\n",
    "#         with open(os.path.join(directory_path, f'query_{i}.pkl'), 'rb') as f:\n",
    "#             shared_queries.append(pickle.load(f))\n",
    "# analysis_expr1(datasets, trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0928b7-978d-4dd1-8dd0-f22d14e9c20b",
   "metadata": {},
   "source": [
    "## Expr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f61d7-a8c5-45e6-820c-c3b13c2f5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_expr2(num_strings, string_length=450):\n",
    "    \"\"\"Generate a dataset with specified number of strings, each of fixed length.\"\"\"\n",
    "    alphabet = string.ascii_uppercase  # English uppercase alphabet\n",
    "    dataset = [\n",
    "        ''.join(random.choices(alphabet, k=string_length))\n",
    "        for _ in range(num_strings)\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "def sample_dataset(dataset, sample_percentage):\n",
    "    \"\"\"Take a sample of the dataset with the specified percentage.\"\"\"\n",
    "    sample_size = int(len(dataset) * sample_percentage)\n",
    "    return random.sample(dataset, sample_size)\n",
    "\n",
    "def generate_query_workload(sample, query_count, lower_char_count, upper_char_count):\n",
    "    \"\"\"Generate query workload from the given sample.\"\"\"\n",
    "    queries = []\n",
    "\n",
    "    for _ in range(query_count):\n",
    "        # Select a random string from the sample\n",
    "        random_string = random.choice(sample)\n",
    "\n",
    "        # Ensure the string has at least lower characters for meaningful slicing\n",
    "        while len(random_string) < lower_char_count+1:\n",
    "            random_string = random.choice(sample)\n",
    "\n",
    "        # Choose a random slice of characters from the string\n",
    "        slice1_start = random.randint(0, len(random_string) - lower_char_count)\n",
    "        slice1_length = random.randint(lower_char_count, upper_char_count)\n",
    "        slice1 = random_string[slice1_start:slice1_start + min(slice1_length, len(random_string)-1)]\n",
    "\n",
    "        # Decide on a random gap size\n",
    "        gap_size = random.randint(1, max(1, min(50, len(random_string) - len(slice1) - 1)))\n",
    "        \n",
    "        # Choose another slice of 3-8 characters after the gap\n",
    "        slice2_start = slice1_start + slice1_length + gap_size\n",
    "        if slice2_start + upper_char_count < len(random_string):  # Ensure valid slice range\n",
    "            slice2_length = random.randint(lower_char_count, upper_char_count)\n",
    "            slice2 = random_string[slice2_start:slice2_start + slice2_length]\n",
    "        else:\n",
    "            slice2 = random_string[slice2_start:]  # Handle edge case where slice2 is out of range\n",
    "        \n",
    "        # Create the query string with a regex-style gap\n",
    "        query = f\"{slice1}(.{{0,{gap_size}}}){slice2}\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries\n",
    "\n",
    "def save_dataset(dataset, filename):\n",
    "    \"\"\"Save the dataset to a file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in dataset:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "def save_queries(queries, filename):\n",
    "    \"\"\"Save the query workload to a file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for query in queries:\n",
    "            f.write(query + '\\n')\n",
    "\n",
    "def generate_and_save_datasets(data_dir):\n",
    "    \"\"\"Generate datasets and save them to files.\"\"\"\n",
    "    dataset_sizes = [20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "\n",
    "    for size in dataset_sizes:\n",
    "        dataset = generate_dataset_expr2(size)\n",
    "        save_dataset(dataset, os.path.join(data_dir, f'dataset_{size}.txt'))\n",
    "\n",
    "def generate_and_save_query_workloads(query_dir, data_dir):\n",
    "    \"\"\"Generate query workloads from datasets and save them.\"\"\"\n",
    "    query_sizes = [100, 500, 2_000, 2_500, 5_000]\n",
    "\n",
    "    # Load the 20K dataset to generate workloads\n",
    "    with open(os.path.join(data_dir, 'dataset_20000.txt'), 'r') as f:\n",
    "        dataset_20k = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for query_size in query_sizes:\n",
    "        sample = sample_dataset(dataset_20k, 0.1)  # Take 10% sample\n",
    "        queries = generate_query_workload(sample, query_size, 3, 8)\n",
    "        save_queries(queries, os.path.join(query_dir, f'query_workload_{query_size}.txt'))\n",
    "\n",
    "def generate_and_save_fixed_workload(query_dir, data_dir):\n",
    "    \"\"\"Generate a fixed 1000-query workload for all datasets.\"\"\"\n",
    "    # Generate workloads for the original datasets\n",
    "    dataset_sizes = [20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "\n",
    "    for size in dataset_sizes:\n",
    "        with open(os.path.join(data_dir, f'dataset_{size}.txt'), 'r') as f:\n",
    "            dataset = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        sample = sample_dataset(dataset, 0.1)  # Take 10% sample\n",
    "        queries = generate_query_workload(sample, 1000, 3, 8)  # Fixed 1000 queries\n",
    "        save_queries(queries, os.path.join(query_dir, f'query_workload_1000_for_{size}.txt'))\n",
    "\n",
    "def generate_expr2():\n",
    "    query_dir = os.path.join(DATA_DIR, 'expr2', 'queries')\n",
    "    os.makedirs(query_dir, exist_ok=True)\n",
    "    data_dir = os.path.join(DATA_DIR, 'expr2', 'datasets')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    # Generate datasets\n",
    "    print(\"Generating datasets...\")\n",
    "    generate_and_save_datasets(data_dir)\n",
    "\n",
    "    # Generate query workloads with fixed 20K dataset\n",
    "    print(\"Generating query workloads with fixed 20K dataset...\")\n",
    "    generate_and_save_query_workloads(query_dir, data_dir)\n",
    "\n",
    "    # Generate fixed 1000-query workloads for all datasets\n",
    "    print(\"Generating fixed 1000-query workloads...\")\n",
    "    generate_and_save_fixed_workload(query_dir, data_dir)\n",
    "\n",
    "    print(\"All datasets and query workloads generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450d97e-e07e-4f93-b5d9-215bfd4c6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_expr2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306b7e-de36-4077-b316-0e1c1b1f653a",
   "metadata": {},
   "source": [
    "## Expr 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04350e-626b-48bb-afa0-1a5a305bd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_expr4(alphabet, num_records=5000):\n",
    "    \"\"\"Generate a dataset of strings using the given alphabet.\"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        current_string = []\n",
    "        s = len(alphabet)  # Alphabet size\n",
    "\n",
    "        # Generate the string character-by-character with 1/(2*s) chance of stopping\n",
    "        while True:\n",
    "            char = random.choice(alphabet)\n",
    "            current_string.append(char)\n",
    "\n",
    "            # End generation with a 1/(2*s) probability after adding a character\n",
    "            if random.random() < 1 / (2 * s):\n",
    "                break\n",
    "\n",
    "        dataset.append(''.join(current_string))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_expr4():\n",
    "    # Define alphabets for Rob datasets\n",
    "    rob_alphabets = {\n",
    "        'Rob01': string.ascii_uppercase[:4],  # A-D\n",
    "        'Rob02': string.ascii_uppercase[:8],  # A-H\n",
    "        'Rob03': string.ascii_uppercase[:12],  # A-L\n",
    "        'Rob04': string.ascii_uppercase[:16],  # A-P\n",
    "    }\n",
    "\n",
    "    data_dir = os.path.join('expr4', 'datasets')\n",
    "    query_dir = os.path.join('expr4', 'queries')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "    # Generate datasets for Rob01 to Rob04\n",
    "    for rob_name, alphabet in rob_alphabets.items():\n",
    "        dataset = generate_dataset_expr4(alphabet)\n",
    "        save_dataset(dataset, os.path.join(data_dir, f'{rob_name}.txt'))\n",
    "\n",
    "        # Generate 10%, 30%, and 50% samples and their query workloads\n",
    "        for sample_pct in [0.1, 0.3, 0.5]:\n",
    "            sample = sample_dataset(dataset, sample_pct)\n",
    "            queries = generate_query_workload(sample, int(len(dataset) * sample_pct), 1, 5)\n",
    "            save_queries(queries, os.path.join(query_dir, f'{rob_name}_queries_{int(sample_pct*100)}pct.txt'))\n",
    "        # Generate 2% test query set\n",
    "        test_sample = sample_dataset(dataset, 0.02)\n",
    "        test_queries = generate_query_workload(sample, int(len(dataset) * 0.02), 1, 5)\n",
    "        save_queries(test_queries, os.path.join(query_dir, f'{rob_name}_test_queries_2pct.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03ecb8-d8d9-4a1d-a3af-175bda6d0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_expr4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
