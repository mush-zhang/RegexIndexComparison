{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb3d5c1-715f-47ef-9ec0-0cb72a27bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library modules\n",
    "import os, sys, errno, json, ssl, time\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bed2ab-8fc4-4e98-97a4-6235a9a280a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/protein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8cf307-4540-4880-83b4-d4d85abd63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.ebi.ac.uk/interpro/api\"\n",
    "PFAM_INDI_URL_PREFIX = BASE_URL + '/protein/UniProt/entry/pfam/'\n",
    "PROTEIN_URL_PREFIX = BASE_URL + '/protein/uniprot/'\n",
    "PFAM_LIST_URL = BASE_URL + '/entry/all/pfam'\n",
    "\n",
    "PROSITE_LIST_URL = BASE_URL + '/entry/all/prosite'\n",
    "PROSITE_URL_FORMAT = f'https://prosite.expasy.org/{}.txt' # fill in something starting with 'PS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a83476-e5c9-4d1b-bb58-8b3d66105e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_payload(curr_url, context, data_type='json'):\n",
    "    last_page = False\n",
    "    next_url = curr_url\n",
    "    attempts = 0\n",
    "    content_type = 'application/json' if data_type == 'json' else 'text/plain'\n",
    "    while attempts < 3:\n",
    "        try:\n",
    "            req = request.Request(curr_url, headers={\"Accept\": content_type})\n",
    "            res = request.urlopen(req, context=context)\n",
    "            # If the API times out due a long running query\n",
    "            if res.status == 408:\n",
    "                # wait just over a minute\n",
    "                time.sleep(61)\n",
    "                # then continue this loop with the same URL\n",
    "                return False, curr_url, None\n",
    "            elif res.status == 204:\n",
    "                #no data so leave loop\n",
    "                return True, curr_url, None\n",
    "            if data_type == 'json':\n",
    "                payload = json.loads(res.read().decode())\n",
    "                next_url = payload.get('next')\n",
    "                if not next_url:\n",
    "                    last_page = True\n",
    "                return last_page, next_url, payload\n",
    "            elif data_type == 'txt':\n",
    "                return True, None, res.read().decode('utf-8')\n",
    "            else:\n",
    "                sys.stderr.write(\"Unknown data type: \" + data_type)\n",
    "                return last_page, None, None\n",
    "        \n",
    "        except HTTPError as e:\n",
    "            if e.code == 408:\n",
    "                time.sleep(61)\n",
    "                continue\n",
    "            else:\n",
    "                # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "                if attempts < 3:\n",
    "                    attempts += 1\n",
    "                    time.sleep(61)\n",
    "                    continue\n",
    "                else:\n",
    "                    sys.stderr.write(\"LAST URL: \" + next_url)\n",
    "                    raise e\n",
    "    return last_page, next_url, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f58cf4-aee2-4e4b-b142-5dfdd0c8339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_family_full_list(start_url, upper_limit=1000000):\n",
    "    id_result = []\n",
    "    \n",
    "    #disable SSL verification to avoid config issues\n",
    "    context = ssl._create_unverified_context()\n",
    "\n",
    "    next_url = start_url\n",
    "    last_page = False\n",
    "\n",
    "    attempts = 0\n",
    "    while not last_page and len(id_result) < upper_limit:\n",
    "        last_page, next_url, payload = try_get_payload(next_url)\n",
    "        if payload is None:\n",
    "            continue\n",
    "        for i, item in enumerate(payload[\"results\"]):\n",
    "            id_result.append(item[\"metadata\"][\"accession\"])\n",
    "            if len(id_result) % 1000 == 0:\n",
    "                sys.stdout.write(f'{len(id_result)}-th read: {id_result[-1]}\\n')\n",
    "        # Don't overload the server, give it time before asking for more\n",
    "        if next_url:\n",
    "            time.sleep(1)\n",
    "    return id_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb9b9d0-4d00-400c-a095-868128889766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_fname = 'pfam_ids.pkl'\n",
    "if os.path.exists(pfam_fname):\n",
    "    with open(pfam_fname, 'rb') as f:\n",
    "        id_result = pickle.load(f)\n",
    "else:\n",
    "    id_result = get_family_full_list(PFAM_LIST_URL)\n",
    "    with open(pfam_fname, 'wb') as f:\n",
    "        pickle.dump(id_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23cfaffb-2aeb-4bfe-b021-59ca266eb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21979\n"
     ]
    }
   ],
   "source": [
    "print(len(id_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45dae513-be31-4048-ba08-3836192d63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_ids(start_url, entry_set, entry_list, upper_limit=100000):   \n",
    "    #disable SSL verification to avoid config issues\n",
    "    context = ssl._create_unverified_context()\n",
    "\n",
    "    next_url = start_url\n",
    "    last_page = False\n",
    "\n",
    "    attempts = 0\n",
    "    prev_length  = len(entry_list)\n",
    "    while not last_page and len(entry_set) < upper_limit:\n",
    "        last_page, next_url, payload = try_get_payload(next_url, context)\n",
    "        if payload is None:\n",
    "            continue\n",
    "        for i, item in enumerate(payload[\"results\"]):\n",
    "            curr_protein = item[\"metadata\"][\"accession\"]\n",
    "            if curr_protein not in entry_set:\n",
    "                entry_set.add(curr_protein)\n",
    "                entry_list.append(curr_protein)\n",
    "\n",
    "        if len(entry_list) - prev_length > 1000:\n",
    "            sys.stdout.write(f'{len(entry_list)}-th read: {entry_list[-1]}\\n')\n",
    "            prev_length = len(entry_list)\n",
    "        # Don't overload the server, give it time before asking for more\n",
    "        if next_url:\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb473937-b07e-4c9f-a272-b0ac77f15c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(start_url, protein_set, protein_list):   \n",
    "    #disable SSL verification to avoid config issues\n",
    "    context = ssl._create_unverified_context()\n",
    "\n",
    "    next_url = start_url\n",
    "    last_page = False\n",
    "\n",
    "    attempts = 0\n",
    "    prev_length  = len(protein_list)\n",
    "    while not last_page and len(protein_set) < 100000:\n",
    "        last_page, next_url, payload = try_get_payload(next_url, context)\n",
    "        if payload is None:\n",
    "            continue\n",
    "        for i, item in enumerate(payload[\"results\"]):\n",
    "            curr_protein = item[\"metadata\"][\"accession\"]\n",
    "            if curr_protein not in protein_set:\n",
    "                protein_set.add(curr_protein)\n",
    "                protein_list.append(curr_protein)\n",
    "\n",
    "        if len(protein_list) - prev_length > 1000:\n",
    "            sys.stdout.write(f'{len(protein_list)}-th read: {protein_list[-1]}\\n')\n",
    "            prev_length = len(protein_list)\n",
    "        # Don't overload the server, give it time before asking for more\n",
    "        if next_url:\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec1a3f-5a54-4669-85be-e9cf81f45bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020-th read: A0A060X6M9\n",
      "2040-th read: A0A075DN36\n",
      "3060-th read: A0A087W0E3\n",
      "4080-th read: A0A091CUA7\n",
      "5100-th read: A0A091IRM8\n",
      "6120-th read: A0A091P9H5\n",
      "7140-th read: A0A091UU47\n",
      "8160-th read: A0A093GHP7\n",
      "9180-th read: A0A093RY33\n",
      "10200-th read: A0A099ZFK6\n",
      "11220-th read: A0A0B1SQN2\n",
      "12240-th read: A0A0C6G6Q4\n",
      "13260-th read: A0A0F7DH26\n",
      "14280-th read: A0A0K0CSW9\n",
      "15300-th read: A0A0L7LBE2\n",
      "16320-th read: A0A0N4TML8\n",
      "17340-th read: A0A0N8JY23\n",
      "18360-th read: A0A0P7XSR9\n",
      "19380-th read: A0A0S2CC36\n",
      "20400-th read: A0A0V1EIK6\n",
      "21420-th read: A0A142BLT0\n",
      "22440-th read: A0A161HKC6\n",
      "23460-th read: A0A182F973\n",
      "24480-th read: A0A182V6M9\n",
      "25500-th read: A0A183TRZ0\n",
      "26520-th read: A0A195DRH0\n",
      "27540-th read: A0A1A8BRR1\n",
      "28560-th read: A0A1A8MTI9\n",
      "29580-th read: A0A1B0GEF6\n",
      "30600-th read: A0A1B6MTP2\n",
      "31620-th read: A0A1I7SD05\n",
      "32640-th read: A0A1I8JQC7\n",
      "33660-th read: A0A1L8HE61\n",
      "34680-th read: A0A1S3G1A5\n"
     ]
    }
   ],
   "source": [
    "protein_list_fn = 'protein_list'\n",
    "protein_set_fn = 'protein_set'\n",
    "if os.path.exists(f'{protein_list_fn}.pkl') and os.path.exists(f'{protein_set_fn}.pkl'):\n",
    "    with open(f'{protein_set_fn}.pkl', 'rb') as f:\n",
    "        protein_set = pickle.load(f)\n",
    "    with open(f'{protein_list_fn}.pkl', 'rb') as f:\n",
    "        protein_list = pickle.load(f)\n",
    "else:\n",
    "    protein_set = set()\n",
    "    protein_list = []\n",
    "    for pfam_id in id_result:\n",
    "        start_url = PFAM_INDI_URL_PREFIX + pfam_id\n",
    "        get_entry_ids(start_url, protein_set, protein_list)\n",
    "        with open('protein_list.pkl', 'wb') as f:\n",
    "            pickle.dump(protein_list, f)\n",
    "        with open('protein_set.pkl', 'wb') as f:\n",
    "            pickle.dump(protein_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9472c9-64b1-487b-b6b3-d45f45a2b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(protein_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55483fd1-8f3b-4eba-8243-3bcbe9cea74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(url_addr):   \n",
    "    #disable SSL verification to avoid config issues\n",
    "    context = ssl._create_unverified_context()\n",
    "    _, _, payload = try_get_payload(url_addr, context)\n",
    "    if payload is None:\n",
    "        continue\n",
    "    curr_seq = payload[\"metadata\"][\"sequence\"]\n",
    "    time.sleep(1)\n",
    "    return curr_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1ef36-1209-44bf-a202-5b56e48f4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'sequences.txt'), 'w') as f:\n",
    "    for protein_id in protein_list:\n",
    "        start_url = PROTEIN_URL_PREFIX + protein_id\n",
    "        seq = get_sequence(start_url)\n",
    "        f.write(seq + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5a317-f071-452d-b0f2-edf29924a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern(url_addr):\n",
    "    #disable SSL verification to avoid config issues\n",
    "    context = ssl._create_unverified_context()\n",
    "    _, _, payload = try_get_payload(url_addr, context, data_type='txt')\n",
    "    if payload is None:\n",
    "        return None\n",
    "    time.sleep(1)\n",
    "\n",
    "    # get raw pattern\n",
    "    curr_pattern = ''\n",
    "    start_patt = 'PA   '\n",
    "    lines = txt_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith(start_patt)\n",
    "            curr_pattern += line[len(start_patt):]\n",
    "    if len(curr_pattern) == 0:\n",
    "        # contains no pattern; only matrix\n",
    "        return None\n",
    "    return curr_pattern[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8881f-2886-419e-9d8e-59899acd9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us get queries\n",
    "prosite_fname = 'prosite_ids.pkl'\n",
    "if os.path.exists(prosite_fname):\n",
    "    with open(prosite_fname, 'rb') as f:\n",
    "        prosite_id_result = pickle.load(f)\n",
    "else:\n",
    "    prosite_id_result = get_family_full_list(PROSITE_LIST_URL, upper_limit=1000)\n",
    "    with open(prosite_fname, 'wb') as f:\n",
    "        pickle.dump(prosite_id_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83e0f1-d477-49ad-b2f2-249f3d0616c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prosite_fname = 'raw_prosite.pkl'\n",
    "if os.path.exists(raw_prosite_fname):\n",
    "    with open(raw_prosite_fname, 'rb') as f:\n",
    "        raw_prosite = pickle.load(f)\n",
    "else:\n",
    "    raw_prosite = []\n",
    "    for prosite_id in prosite_list:\n",
    "        start_url = PROSITE_URL_FORMAT.format(prosite_id)\n",
    "        pat = get_pattern(start_url)\n",
    "        raw_prosite.append(pat)\n",
    "    with open(raw_prosite_fname, 'wb') as f:\n",
    "        pickle.dump(raw_prosite, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eab0f-e895-43fd-97cc-a464ae7b2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prosite_to_regex(raw_prosite):\n",
    "    reg = raw_prosite.replace('-', '')\n",
    "    reg = reg.replace('x', '.')\n",
    "    reg = reg.replace('{', '[^')\n",
    "    reg = reg.replace('}', ']')\n",
    "    reg = reg.replace('(', '{')\n",
    "    reg = reg.replace(')', '}')\n",
    "    reg = reg.replace('<', '^')\n",
    "    reg = reg.replace('>', '$')\n",
    "    # special case\n",
    "    if reg.endswith('$]'):\n",
    "        reg = reg[:-2] + ']|\\z'\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95965210-b538-4b76-b9fa-cd630b99ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_fn = os.path.join(DATA_DIR, 'prosites.txt')\n",
    "if not os.path.exists(query_fn):\n",
    "    with open(query_fn, 'w') as f:\n",
    "        for pat in raw_prosite:\n",
    "            pat_reg = prosite_to_regex(pat)\n",
    "            f.write(pat_reg + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reg_idx]",
   "language": "python",
   "name": "conda-env-reg_idx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
